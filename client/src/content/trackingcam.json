{
	"title": "Colored-Object Tracking Camera",
	"overview": "Designed and implemented a camera mount which tracks objects based on color. Using an FPGA, we process an input video stream using various color space conversions, filtering based on HSV formatted threshold values and recording the vector of the center of the object with respect to the center of the frame.",
	"motivation": "I have always been a participant in various action sports and by extension, the videos athletes make to showcase their skills. In order to film various actions, it requires another individual to follow and film, which introduces a multitasking issue for the filmer, e.g. riding close to the target, focusing on their own riding while focusing on where the target is, and ensuring the camera is recording the target without missing a moment. So I thought that if you could have a camera that automatically follows the target, the filmer only needs to focus on his own riding and being close which greatly reduces the complexity of the task.",
	"implementation": [
		{
			"subheader": "Input Video Stream and Processing",
			"text": "Using composite video from a security camera, the signal goes through the builtin TV decoder chip on the Altera DE2 development board. The pixel stream goes through a series of color space converters to a final HSV formatted signal. The pixel is then filtered using a threshold range calculated from the average color of a range of pixels of the target captured during initialization. The pixel is then labeled with a 1 for passing the threshold and 0 for failing."
		},
		{
			"subheader": "Position of Colored Object in Frame",
			"text": "From the threshold representation of the pixel, the system records the vertical and horizontal index of the first and last pixels that pass the threshold in the frame. The system gets the centroid of the object by averaging these two pixels indexes. This value is then stored in memory to be accessed in later processing."
		},
		{
			"subheader": "Position Tracking",
			"text": "The implemented Nios2 CPU processes the center of the target stored in memory, converts it to a vector with respect to the center of the frame, and using measured values of signal to servo motor angular displacement, stores the horizontal and vertical signals in memory. These values are read by respective vertical and horizontal Pulsed Width Modulators implemented as FPGA components. These signals output to a two-servo gimbal setup that orients the camera to center the object in the frame."
		},
		{
			"subheader": "Setting Target Color to be Tracked",
			"text": "Flipping a switch on the board initiates our initialization phase where a red square appears on the output screen. To set the color, you center your object in the square and while holding the target, flipping the switch down stores the average of a range of pixels within that square. This range is then used to calculate the threshold for the tracking process."
		},
		{
			"subheader": "Monitor Output",
			"text": "The output of the system comes for our image processing component to an output VGA connection. The monitor shows what the camera’s view for debugging, the red square for initializing target, and also outputs the filtered image in real time for debugging purposes as well as demonstrating what the system sees while tracking. The filtered image comes from the filtered pixel stream used for determining the target’s displacement in the frame. The system outputs white pixels for passing and black pixels for failing the filter."
		},
		{
			"subheader": "Movement Lock",
			"text": "Two switches were implemented to lock either vertical or horizontal movement. This feature was implemented for more linear panning and tracking. Flipping both stopped both motors, which was used to demonstrate the filtering and tracking process on the monitor."
		}
	],
	"future": [
		{
			"item": "Instead of tracking color, utilize openCV to train a neural network using videos, then implementing it on an FPGA."
		},
		{
			"item": "Using a gopro as input video (HDMI) so that instead of implementing a separate camera unit for tracking, we can use the gopro for both recording and using the live stream for tracking."
		},
		{
			"item": "Implement a mobile application to interface with the system, viewing the live video stream as well as choosing the target to be tracked."
		}
	],
	"utilized": [
		{ "skill": "C programming," },
		{ "skill": "FPGA," },
		{ "skill": "VHDL," },
		{ "skill": "MATLAB," },
		{ "skill": "Circuit Design" }
	]
}
